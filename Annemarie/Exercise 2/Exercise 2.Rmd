---
title: "Exercise 2"
author: "Annemarie Timmers"
date: "23-10-2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Aim

As I am learning about machine learning in another course, I want to see how the number of folds in k-fold cross-validation influences the mean squared error by doing a Monte Carlo simulation. I will try 3, 5 and 10 folds and plot the results. 

### Set up

#### Fixing the random seed

I fix the seed of the random number generator so that the results can be reproduced and the same results are obtained if the code in this document is ran again. 

```{r}
set.seed(123)
```

#### Loading the required packages

```{r warning = FALSE, message = FALSE}
library(mlbench) #dataset
library(tidyverse)
```

### Load the dataset

I use the `BostonHousing` dataset that was published by Harrison and Rubinfield in 1979. The data was collected in 1970 by the U.S. Census Service in the Boston Standard Metropolitan Statistical Area and contains 506 census tracts (geographical areas). The dependent variable is `medv`, which is the median value of the housing prices in the respective census tracts. I choose `rm` and `lstat` as predictors, which are the average number of rooms per house in the tracts and the percentage of the population of the tracts that is of a lower socioeconomic status, respectively.

I am not really interested in the theoretical implications of the model, but these predictors were chosen as the have highest correlation with the dependent variable. 

I remove the missing values for easy processing. 

```{r results = 'hide'}
data(BostonHousing) %>%
  na.omit #and get rid of missing values
```

### Cross-validation

I first need a function that actually does cross-validation. This means 

```{r}
#make a function to calculate the mean squared error
mse <- function(obs, pred){
  se <- (obs - pred)^2 #squared difference
  return(mean(se)) #and then the mean of that
}

#make a function that does cross validation for the model of interest, where I can alter the number of folds
cross_validate <-function(folds){
  
  #first split the dataset up 
  n_samples  <- nrow(BostonHousing)
  select_vec <- rep(1:folds, length.out = n_samples)
  splits <- BostonHousing %>% 
    mutate(set = sample(select_vec))
  
  mse_test <- NULL #storage
  
  #for loop to calculate the mse for each of the test sets in the folds
  for (i in 1:folds) { # Start for-loop over folds
    #not great efficiency, but alas
    fold <- splits$set
    #specify the train and test set for the current iteration
    index <- which(fold == i, arr.ind = TRUE)
    test  <- BostonHousing[index, ]
    train <- BostonHousing[-index, ]
    #run the model on the train set
    train_model <- lm(medv ~ rm + lstat, data = train)
    #predict new values in the test set
    test$medv_pred <- predict(train_model, newdata = test)
    #and calculate the mse
    mse_test[i] <- mse(test$medv, test$medv_pred)
  } 
  return(mean(mse_test)) #I would want all 10 values, but that is not really   what I'm interested in and it makes what comes later run nicer
}

#test the function
cross_validate(10) #seems to run smoothly
```

### Monte Carlo simulation

In order to get an indication of the effect of the number of folds, I repeat the process 1000 times, storing the mean of each cross-validation and plotting the results.

```{r}
set.seed(123)
nsim <- 1000 #number of simulations

#first do 10 folds
mc_cv10 <- NULL #storage
for (i in 1:nsim) {
  mc_cv10[i] <- cross_validate(10)
}

#then 5
mc_cv5 <- NULL #storage
for (i in 1:nsim) {
  mc_cv5[i]  <- cross_validate(5)
}

#finally just 3
mc_cv3 <- NULL #storage
for (i in 1:nsim) {
  mc_cv3[i]  <- cross_validate(3)
}

#plot the results
boxplot(mc_cv3, mc_cv5, mc_cv10, 
        main = "MSE for 3, 5 and 10 folds",
        col = c("yellow", "red", "blue"),
        ylab = "MSE", 
        xlab = "Number of folds",
        names = c("3", "5", "10"))
```

### Results
The mean MSE over the simulations is the same, regardless of the number of folds. However, increasing the number of folds does seem to lower the spread of the mean squared errors. 

### Replication with another seed

```{r}
set.seed(321)

mc_cv10 <- NULL
for (i in 1:nsim) {
  mc_cv10[i] <- mean(cross_validate(10))
}

mc_cv5 <- NULL
for (i in 1:nsim) {
  mc_cv5[i] <- mean(cross_validate(5))
}

mc_cv3 <- NULL
for (i in 1:nsim) {
  mc_cv3[i] <- mean(cross_validate(3))
}

#plot the results
boxplot(mc_cv3, mc_cv5, mc_cv10, 
        col = c("yellow", "red", "blue"),
        main = "MSE for 3, 5 and 10 folds",
        ylab = "MSE", 
        xlab = "Number of folds",
        names = c("3", "5", "10"))
```

### Conclusion

Repeating the simulation with another seed alters the boxplots a little, but the results still indicate the same thing: increasing the number of folds renders the same MSE over the simulations, but it does make the results more stable. 

### Session info

```{r session}
sessionInfo()
```

